/**
 * useSpeechRecognition - Voice recognition hook for G Studio
 *
 * Supports both English and Persian (Farsi) languages
 * Uses Web Speech API for online and Vosk for offline recognition
 */

import { useState, useCallback, useRef, useEffect, useMemo } from "react";

// Types
export type SpeechLanguage = "fa" | "en";

export interface SpeechRecognitionOptions {
  isListening?: boolean;
  language?: string;
  voiceCommandMode?: boolean;
  /** Optional callback for sending recognized text */
  onSend?: (text: string) => void | Promise<void>;
  onListeningChange?: (b: boolean) => void;
  defaultLanguage?: SpeechLanguage;
  autoDetectLanguage?: boolean;
  continuousMode?: boolean;
  offlineMode?: boolean;
  getActiveFile?: () => string | null;
  getSelectedCode?: () => string | null;
  getCurrentFileContent?: () => string | null;
  getCursorPosition?: () => { line: number; column: number };
  getProjectPath?: () => string;
}

export interface SpeechRecognitionState {
  isListening: boolean;
  isProcessing: boolean;
  transcript: string;
  interimTranscript: string;
  language: SpeechLanguage;
  confidence: number;
  error: string | null;
}

export interface CommandResult {
  success: boolean;
  command: string;
  action?: string;
  error?: string;
}

export interface UseSpeechRecognitionReturn {
  // State
  isListening: boolean;
  isProcessing: boolean;
  transcript: string;
  interimTranscript: string;
  language: SpeechLanguage;
  confidence: number;
  error: string | null;
  isSupported: boolean;
  lastCommandResult: CommandResult | null;
  hasSpeechText: boolean;
  // Actions
  startListening: () => void;
  stopListening: () => void;
  toggleListening: () => void;
  setLanguage: (lang: SpeechLanguage) => void;
  clearTranscript: () => void;
  clearSpeechState: () => void;
  clearError: () => void;
  setError: (error: string | null) => void;
}

// Language code mappings
const LANGUAGE_CODES: Record<SpeechLanguage, string> = {
  en: "en-US",
  fa: "fa-IR",
};

// Persian character detection
const containsPersian = (text: string): boolean => {
  const persianRegex = /[\u0600-\u06FF\u0750-\u077F\uFB50-\uFDFF\uFE70-\uFEFF]/;
  return persianRegex.test(text);
};

// Check browser support
const checkSpeechSupport = (): boolean => {
  if (typeof window === "undefined") return false;
  return !!(
    (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
  );
};

// Get Speech Recognition constructor
const getSpeechRecognition = (): any => {
  if (typeof window === "undefined") return null;
  return (
    (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
  );
};

/**
 * Speech Recognition Hook
 */
export function useSpeechRecognition(
  options: SpeechRecognitionOptions = {},
): UseSpeechRecognitionReturn {
  const {
    defaultLanguage = "en",
    autoDetectLanguage = true,
    continuousMode = false,
    offlineMode = false,
  } = options;

  // State
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [interimTranscript, setInterimTranscript] = useState("");
  const [language, setLanguageState] =
    useState<SpeechLanguage>(defaultLanguage);
  const [confidence, setConfidence] = useState(0);
  const [error, setError] = useState<string | null>(null);
  const [lastCommandResult, setLastCommandResult] =
    useState<CommandResult | null>(null);

  // Refs
  const recognitionRef = useRef<any>(null);
  const isSupported = useMemo(() => checkSpeechSupport(), []);

  // Initialize recognition
  const initRecognition = useCallback(() => {
    const SpeechRecognition = getSpeechRecognition();
    if (!SpeechRecognition) return null;

    const recognition = new SpeechRecognition();
    recognition.continuous = continuousMode;
    recognition.interimResults = true;
    recognition.maxAlternatives = 3;
    recognition.lang = LANGUAGE_CODES[language];

    return recognition;
  }, [continuousMode, language]);

  // Setup recognition handlers
  const setupHandlers = useCallback(
    (recognition: any) => {
      recognition.onstart = () => {
        console.log("[SpeechRecognition] Started listening");
        setIsListening(true);
        setError(null);
      };

      recognition.onend = () => {
        console.log("[SpeechRecognition] Stopped listening");
        setIsListening(false);
        setIsProcessing(false);

        // Auto-restart in continuous mode
        if (continuousMode && recognitionRef.current) {
          try {
            recognitionRef.current.start();
          } catch (e) {
            console.warn("[SpeechRecognition] Could not restart:", e);
          }
        }
      };

      recognition.onresult = (event: any) => {
        let finalTranscript = "";
        let interimText = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          const text = result[0].transcript;

          if (result.isFinal) {
            finalTranscript += text;
            setConfidence(result[0].confidence || 0);

            // Auto-detect language
            if (autoDetectLanguage && containsPersian(text)) {
              setLanguageState("fa");
            }
          } else {
            interimText += text;
          }
        }

        if (finalTranscript) {
          setTranscript((prev) => prev + finalTranscript);
          setIsProcessing(true);

          // Process command
          setLastCommandResult({
            success: true,
            command: finalTranscript.trim(),
          });

          setTimeout(() => setIsProcessing(false), 100);
        }

        setInterimTranscript(interimText);
      };

      recognition.onerror = (event: any) => {
        console.error("[SpeechRecognition] Error:", event.error);

        let errorMessage = "Speech recognition error";
        switch (event.error) {
          case "no-speech":
            errorMessage = "No speech detected. Please try again.";
            break;
          case "audio-capture":
            errorMessage = "Microphone not found or not accessible.";
            break;
          case "not-allowed":
            errorMessage = "Microphone permission denied.";
            break;
          case "network":
            errorMessage = "Network error. Persian may require internet.";
            // Try switching to English for offline
            if (language === "fa" && !offlineMode) {
              setLanguageState("en");
            }
            break;
          case "aborted":
            errorMessage = "Speech recognition aborted.";
            break;
          default:
            errorMessage = `Speech error: ${event.error}`;
        }

        setError(errorMessage);
        setIsListening(false);
      };

      recognition.onnomatch = () => {
        console.warn("[SpeechRecognition] No match found");
        setError("Could not understand. Please speak clearly.");
      };
    },
    [autoDetectLanguage, continuousMode, language, offlineMode],
  );

  // Start listening
  const startListening = useCallback(() => {
    if (!isSupported) {
      setError("Speech recognition is not supported in this browser.");
      return;
    }

    if (isListening) {
      console.warn("[SpeechRecognition] Already listening");
      return;
    }

    try {
      // Create new recognition instance
      const recognition = initRecognition();
      if (!recognition) {
        setError("Could not initialize speech recognition.");
        return;
      }

      setupHandlers(recognition);
      recognitionRef.current = recognition;

      // Request microphone permission and start
      recognition.start();
      setError(null);
    } catch (err) {
      console.error("[SpeechRecognition] Start error:", err);
      setError("Failed to start speech recognition.");
    }
  }, [isSupported, isListening, initRecognition, setupHandlers]);

  // Stop listening
  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
      } catch (e) {
        console.warn("[SpeechRecognition] Stop error:", e);
      }
      recognitionRef.current = null;
    }
    setIsListening(false);
    setInterimTranscript("");
  }, []);

  // Toggle listening
  const toggleListening = useCallback(() => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  }, [isListening, startListening, stopListening]);

  // Set language
  const setLanguage = useCallback(
    (lang: SpeechLanguage) => {
      setLanguageState(lang);

      // Restart recognition with new language if listening
      if (isListening && recognitionRef.current) {
        stopListening();
        setTimeout(() => startListening(), 100);
      }
    },
    [isListening, stopListening, startListening],
  );

  // Clear transcript
  const clearTranscript = useCallback(() => {
    setTranscript("");
    setInterimTranscript("");
  }, []);

  // Clear speech state (alias for clearTranscript for backward compatibility)
  const clearSpeechState = useCallback(() => {
    setTranscript("");
    setInterimTranscript("");
    setError(null);
  }, []);

  // Clear error
  const clearError = useCallback(() => {
    setError(null);
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (e) {
          // Ignore cleanup errors
        }
      }
    };
  }, []);

  return {
    isListening,
    isProcessing,
    transcript,
    interimTranscript,
    language,
    confidence,
    error,
    isSupported,
    lastCommandResult,
    startListening,
    stopListening,
    toggleListening,
    setLanguage,
    clearTranscript,
    clearSpeechState,
    clearError,
    hasSpeechText: transcript.length > 0 || interimTranscript.length > 0,
  };
}

// Context for app-wide speech recognition
import React, { createContext, useContext, ReactNode } from "react";

const SpeechRecognitionContext =
  createContext<UseSpeechRecognitionReturn | null>(null);
export { SpeechRecognitionContext as SpeechContext };

interface SpeechRecognitionProviderProps {
  children: ReactNode;
  options?: SpeechRecognitionOptions;
}

export const SpeechRecognitionProvider: React.FC<
  SpeechRecognitionProviderProps
> = ({ children, options }) => {
  const speechRecognition = useSpeechRecognition(options);
  return (
    <SpeechRecognitionContext.Provider value={speechRecognition}>
      {children}
    </SpeechRecognitionContext.Provider>
  );
};

export const useSpeechRecognitionContext = (): UseSpeechRecognitionReturn => {
  const context = useContext(SpeechRecognitionContext);
  if (!context) {
    throw new Error(
      "useSpeechRecognitionContext must be used within a SpeechRecognitionProvider",
    );
  }
  return context;
};

export default useSpeechRecognition;
